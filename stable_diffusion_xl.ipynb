{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# init\n",
    "import torch\n",
    "\n",
    "# for parameters ui\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "# for config\n",
    "import json\n",
    "# for saving results\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "# for image_grid\n",
    "from PIL import Image\n",
    "# for prompt embeddings\n",
    "from compel import Compel, ReturnedEmbeddingsType\n",
    "\n",
    "# hugging face cache directory\n",
    "CACHE_DIR = \"D:\\HuggingFaceCache\"\n",
    "# config path\n",
    "CONFIG_PATH = \"sdxl_config.json\"\n",
    "\n",
    "# import models, schedulers and etc\n",
    "from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers import EulerDiscreteScheduler, DDIMScheduler, LMSDiscreteScheduler\n",
    "\n",
    "BASE_PIPELINES = {\"StableDiffusionXLPipeline\":StableDiffusionXLPipeline}\n",
    "REFINER_PIPELINES = {\"StableDiffusionXLImg2ImgPipeline\":StableDiffusionXLImg2ImgPipeline}\n",
    "SCHEDULERS = {\"EulerDiscreteScheduler\":EulerDiscreteScheduler, \n",
    "              \"DDIMScheduler\":DDIMScheduler, \n",
    "              \"LMSDiscreteScheduler\":LMSDiscreteScheduler}\n",
    "PRECISION = {\"float16\":torch.float16}\n",
    "\n",
    "class SDXLConfig:\n",
    "    def __init__(self,\n",
    "                 base_pipe_model: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                 refiner_pipe_model: str = \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "                 torch_dtype: str = \"torch.float16\",\n",
    "                 base_pipeline_type: str = \"StableDiffusionXLPipeline\",\n",
    "                 refiner_pipeline_type: str = \"StableDiffusionXLImg2ImgPipeline\",\n",
    "                 scheduler_type: str = \"LMSDiscreteScheduler\",\n",
    "                 variant: str = \"fp16\",\n",
    "                 use_safetensors: bool = True,\n",
    "                 #safety_checker = None\n",
    "                 prompt: str = None,\n",
    "                 prompt_2: str = None,\n",
    "                 negative_prompt: str = None,\n",
    "                 negative_prompt_2: str = None,\n",
    "                 use_compel: bool = False,\n",
    "                 num_inference_steps: int = 40,\n",
    "                 width: int = 768,\n",
    "                 height: int = 768,\n",
    "                 guidance_scale: float = 7.5,\n",
    "                 high_noise_frac: float = 0.8,\n",
    "                 seed: int = 12345,\n",
    "                 use_refiner: bool = False\n",
    "                 ):\n",
    "        self.base_pipe_model = base_pipe_model\n",
    "        self.refiner_pipe_model = refiner_pipe_model\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.base_pipeline_type = base_pipeline_type\n",
    "        self.refiner_pipeline_type = refiner_pipeline_type\n",
    "        self.scheduler_type = scheduler_type\n",
    "        self.variant = variant\n",
    "        self.use_safetensors = use_safetensors\n",
    "        self.prompt = prompt\n",
    "        self.prompt_2 = prompt_2\n",
    "        self.negative_prompt = negative_prompt\n",
    "        self.negative_prompt_2 = negative_prompt_2\n",
    "        self.use_compel = use_compel\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.guidance_scale = guidance_scale\n",
    "        self.high_noise_frac = high_noise_frac\n",
    "        self.seed = seed\n",
    "        self.use_refiner = use_refiner\n",
    "\n",
    "    def to_json(obj):\n",
    "        if isinstance(obj, SDXLConfig):\n",
    "            return obj.__dict__\n",
    "    def from_json(dict: dict):\n",
    "            return SDXLConfig(**dict)\n",
    "    def load_config():\n",
    "         with open(CONFIG_PATH, \"r\") as read_file:\n",
    "            return json.load(read_file, object_hook=SDXLConfig.from_json)\n",
    "    def save_config(self):\n",
    "         with open(CONFIG_PATH, \"w\") as write_file:\n",
    "            json.dump(self, write_file, skipkeys=True, indent=1, default=SDXLConfig.to_json)\n",
    "\n",
    "    def set_ui(self):\n",
    "        # TODO: not best workaround to get variable name\n",
    "        def set_attr(x, f_name): setattr(self, f_name.split('=')[0].split('.')[1], x)\n",
    "\n",
    "        # set ui\n",
    "        style = {'description_width': 'initial'}\n",
    "        interact(set_attr, x=widgets.Text(value=self.base_pipe_model, placeholder='', description='Base model:', style=style), f_name=fixed(f'{self.base_pipe_model=}'))\n",
    "        interact(set_attr, x=widgets.Text(value=self.refiner_pipe_model, placeholder='', description='Refiner model:', style=style), f_name=fixed(f'{self.refiner_pipe_model=}'))\n",
    "        interact(set_attr, x=widgets.Dropdown(options=PRECISION, description='dtype:', style=style), f_name=fixed(f'{self.torch_dtype=}'))\n",
    "\n",
    "        basetype_dropdown = widgets.Dropdown(options=BASE_PIPELINES, description='Base type:', style=style)\n",
    "        display(basetype_dropdown)\n",
    "        reftype_dropdown = widgets.Dropdown(options=REFINER_PIPELINES, description='Refiner type:', style=style)\n",
    "        display(reftype_dropdown)\n",
    "        schedulertype_dropdown = widgets.Dropdown(options=SCHEDULERS, description='Scheduler type:', style=style)\n",
    "        display(schedulertype_dropdown)\n",
    "\n",
    "        interact(set_attr, x=widgets.Textarea(value=self.prompt, placeholder='Type positive1...', description='Prompt1:', style=style), f_name=fixed(f'{self.prompt=}'))\n",
    "        interact(set_attr, x=widgets.Textarea(value=self.prompt_2, placeholder='Type positive2...', description='Prompt2:', style=style), f_name=fixed(f'{self.prompt_2=}'))\n",
    "        negative_prompt1_text_area = widgets.Textarea(value=self.negative_prompt, placeholder='Type negative1...', description='Negative Prompt1:', style = style)\n",
    "        display(negative_prompt1_text_area)\n",
    "        negative_prompt2_text_area = widgets.Textarea(value=self.negative_prompt_2, placeholder='Type negative2...', description='Negative Prompt2:', style = style)\n",
    "        display(negative_prompt2_text_area)\n",
    "        use_compel_checkbox = widgets.Checkbox(value=self.use_compel, description=\"Use Compel\", indent=False, style=style)\n",
    "        display(use_compel_checkbox)\n",
    "\n",
    "        # inference properties\n",
    "        num_inference_steps_slider = widgets.IntSlider(value=self.num_inference_steps, min=10, max=100, step=5, description=\"Num inference steps:\", continuous_update=False, style=style)\n",
    "        display(num_inference_steps_slider)\n",
    "        width_slider = widgets.IntSlider(value=self.width, min=512, max=1024, step=64, description=\"Width:\", continuous_update=False, style=style)\n",
    "        display(width_slider)\n",
    "        height_slider = widgets.IntSlider(value=self.height, min=512, max=1024, step=64, description=\"Height:\", continuous_update=False, style=style)\n",
    "        display(height_slider)\n",
    "        guidance_scale_slider = widgets.FloatSlider(value=self.guidance_scale, min=0, max=10, step=0.25, description=\"Guidance scale:\", continuous_update=False, style=style)\n",
    "        display(guidance_scale_slider)\n",
    "        seed_slider = widgets.IntSlider(value=self.seed, min=0, max=1000000, step=1, description=\"Seed:\", continuous_update=False, style=style)\n",
    "        display(seed_slider)\n",
    "        high_noise_frac_slider = widgets.FloatSlider(value=self.high_noise_frac, min=0, max=1, step=0.05, description=\"High noise frac:\", continuous_update=False, style=style)\n",
    "        display(high_noise_frac_slider)\n",
    "\n",
    "        # refiner\n",
    "        use_refiner_checkbox = widgets.Checkbox(value=self.use_refiner, description=\"Use refiner\", indent=False, style=style)\n",
    "        display(use_refiner_checkbox)\n",
    "\n",
    "# utilities methods\n",
    "def to_cuda(pipe, start_mess, end_mess):\n",
    "    if(torch.cuda.is_available()):\n",
    "        print(start_mess)\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA IS NOT AVAILABLE\")\n",
    "    print(end_mess)\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def postprocess_latent(pipe, latent):\n",
    "    vae_output = pipe.vae.decode(\n",
    "        latent.images / pipe.vae.config.scaling_factor, return_dict=False\n",
    "    )[0].detach()\n",
    "    return pipe.image_processor.postprocess(vae_output, output_type=\"pil\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc29d5117184bef81ab4a0f06ab125a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='stabil', description='Base model:', placeholder='', style=TextStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e3872100ea4ef18ef2c0c6a087781d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='stabilityai/stable-diffusion-xl-refiner-1.0', description='Refiner model:', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc326a74c928450dacb6ced3ceab4b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dtype:', options={'float16': torch.float16}, style=DescriptionStylâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30dc8cd678494c399e17cd6edb039d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Base type:', options={'StableDiffusionXLPipeline': <class 'diffusers.pipelines.stable_diâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6680601c8e46708eda08912c645706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Refiner type:', options={'StableDiffusionXLImg2ImgPipeline': <class 'diffusers.pipelinesâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c3c19c119b4350b678b196577acf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Scheduler type:', options={'EulerDiscreteScheduler': <class 'diffusers.schedulers.scheduâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cda44e3e074ba8bef3caec78645b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='', description='Prompt1:', placeholder='Type positive1...', style=TextStâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf23a54c50047939f2e5732dbc462ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='', description='Prompt2:', placeholder='Type positive2...', style=TextStâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5406abed714f75baf13f06585c1e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Negative Prompt1:', placeholder='Type negative1...', style=TextStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34512e3774eb47d3977e6b521a2179a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Negative Prompt2:', placeholder='Type negative2...', style=TextStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0510982eb5403eb445d77146396393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Use Compel', indent=False, style=CheckboxStyle(description_width='initial')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8d48b3d61e483bab0cf7ff7af2b638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=40, continuous_update=False, description='Num inference steps:', min=10, step=5, style=SliderSâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66c98c801f546bea9331335b8639133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=768, continuous_update=False, description='Width:', max=1024, min=512, step=64, style=SliderStâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3310d46c7df4676a42832324ff58f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=768, continuous_update=False, description='Height:', max=1024, min=512, step=64, style=SliderSâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fac327e94f4a7e8b3bc9e7d5a50ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=7.5, continuous_update=False, description='Guidance scale:', max=10.0, step=0.25, style=Slidâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc49ce036493454a81708a4255efa213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=12345, continuous_update=False, description='Seed:', max=1000000, style=SliderStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1c2c2dd9a54f9d8aaea32434a689c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.8, continuous_update=False, description='High noise frac:', max=1.0, step=0.05, style=Slidâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25837cbc607488d9081c5f278196ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Use refiner', indent=False, style=CheckboxStyle(description_width='initial'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ui\n",
    "config: SDXLConfig = SDXLConfig.load_config()\n",
    "config.set_ui()\n",
    "config.save_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.save_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline, set schelduler, to cuda, compel init and etc\n",
    "base_pipe_model = config.base_pipeline_type.from_pretrained(config.base_pipe_model, cache_dir=CACHE_DIR, \n",
    "                                               torch_dtype=config.torch_dtype,\n",
    "                                               variant=variant,\n",
    "                                               use_safetensors=use_safetensors)\n",
    "\n",
    "\n",
    "scheduler = scheduler_type.from_config(base_pipe_model.scheduler.config)\n",
    "base_pipe_model.scheduler = scheduler\n",
    "#base_pipe.unet = torch.compile(base_pipe.unet, mode=\"reduce-overhead\", fullgraph=True) # not for windows\n",
    "\n",
    "# base pipeline to CUDA\n",
    "to_cuda(base_pipe_model, \"Base -> CUDA started\", \"Base -> CUDA finished\")\n",
    "\n",
    "# Compels init (TODO: maybe should move \"generate image\" cell)\n",
    "base_compel_1 = Compel(\n",
    "    tokenizer=base_pipe_model.tokenizer,\n",
    "    text_encoder=base_pipe_model.text_encoder,\n",
    "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "    requires_pooled=False,\n",
    ")\n",
    "base_compel_2 = Compel(\n",
    "    tokenizer=base_pipe_model.tokenizer_2,\n",
    "    text_encoder=base_pipe_model.text_encoder_2,\n",
    "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "    requires_pooled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate image\n",
    "\n",
    "# variables from ui\n",
    "prompt = str(prompt1_text_area.value)\n",
    "prompt_2 = str(prompt2_text_area.value)\n",
    "negative_prompt = str(negative_prompt1_text_area.value)\n",
    "negative_prompt_2 = str(negative_prompt2_text_area.value)\n",
    "print(f\"{prompt}\\n{prompt_2}\\n{negative_prompt}\\n{negative_prompt_2}\")\n",
    "\n",
    "use_compel = use_compel_checkbox.value\n",
    "num_inference_steps=num_inference_steps_slider.value\n",
    "guidance_scale=guidance_scale_slider.value\n",
    "use_refiner= \"latent\" if use_refiner_checkbox.value else \"pil\"\n",
    "high_noise_frac=high_noise_frac_slider.value\n",
    "width = width_slider.value\n",
    "height = height_slider.value\n",
    "seed = seed_slider.value\n",
    "\n",
    "config = SDXLConfig\n",
    "\n",
    "# set embeds\n",
    "base_positive_prompt_embeds_1 = base_compel_1(prompt)\n",
    "base_positive_prompt_embeds_2, base_positive_prompt_pooled = base_compel_2(prompt_2)\n",
    "base_negative_prompt_embeds_1 = base_compel_1(negative_prompt)\n",
    "base_negative_prompt_embeds_2, base_negative_prompt_pooled = base_compel_2(negative_prompt_2)\n",
    "\n",
    "# Pad the conditioning tensors to ensure thet they all have the same length\n",
    "(base_positive_prompt_embeds_2, base_negative_prompt_embeds_2) = base_compel_2.pad_conditioning_tensors_to_same_length([base_positive_prompt_embeds_2, base_negative_prompt_embeds_2])\n",
    "\n",
    "# Concatenate the cconditioning tensors corresponding to both the set of prompts\n",
    "base_positive_prompt_embeds = torch.cat((base_positive_prompt_embeds_1, base_positive_prompt_embeds_2), dim=-1)\n",
    "base_negative_prompt_embeds = torch.cat((base_negative_prompt_embeds_1, base_negative_prompt_embeds_2), dim=-1)\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "\n",
    "# base\n",
    "base_output = base_pipe_model(prompt=prompt if prompt != \"\" and not use_compel else None,\n",
    "                        prompt_2 = prompt_2 if prompt_2 != \"\" and not use_compel else None,\n",
    "                        negative_prompt = negative_prompt if negative_prompt != \"\" and not use_compel else None,\n",
    "                        negative_prompt_2 = negative_prompt_2 if negative_prompt_2 != \"\" and not use_compel else None,\n",
    "                        prompt_embeds=base_positive_prompt_embeds if base_positive_prompt_embeds != \"\" and use_compel else None,\n",
    "                        pooled_prompt_embeds=base_positive_prompt_pooled if base_positive_prompt_pooled != \"\" and use_compel else None,\n",
    "                        negative_prompt_embeds=base_negative_prompt_embeds if base_negative_prompt_embeds != \"\" and use_compel else None,\n",
    "                        negative_pooled_prompt_embeds=base_negative_prompt_pooled if base_negative_prompt_pooled != \"\" and use_compel else None,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                        generator=generator,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                        output_type=use_refiner,\n",
    "                        denoising_end=high_noise_frac,\n",
    "                        width=width,\n",
    "                        height=height)\n",
    "\n",
    "config.save_config()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refiner\n",
    "\n",
    "refiner_pipe_model = refiner_pipeline_type.from_pretrained(refiner_model, cache_dir=CACHE_DIR, \n",
    "                                    torch_dtype=torch_dtype,\n",
    "                                    variant=variant,\n",
    "                                    use_safetensors=use_safetensors,\n",
    "                                    vae=base_pipe_model.vae,\n",
    "                                    text_encoder_2=base_pipe_model.text_encoder_2)\n",
    "\n",
    "refiner_pipe_model.scheduler = scheduler\n",
    "#refiner_pipe.unet = torch.compile(refiner_pipe.unet, mode=\"reduce-overhead\", fullgraph=True) # not for windows\n",
    "\n",
    "# refiner pipeline to CUDA\n",
    "to_cuda(base_pipe_model, \"Refiner -> CUDA started\", \"Refiner -> CUDA finished\")\n",
    "\n",
    "\n",
    "image_refined = refiner_pipe_model(prompt=prompt if prompt != \"\" and not use_compel else None,\n",
    "                             prompt_2 = prompt_2 if prompt_2 != \"\" and not use_compel else None,\n",
    "                             negative_prompt = negative_prompt if negative_prompt != \"\" and not use_compel else None,\n",
    "                             negative_prompt_2 = negative_prompt_2 if negative_prompt_2 != \"\" and not use_compel else None,\n",
    "                             prompt_embeds=base_positive_prompt_embeds if base_positive_prompt_embeds != \"\" and use_compel else None,\n",
    "                             pooled_prompt_embeds=base_positive_prompt_pooled if base_positive_prompt_pooled != \"\" and use_compel else None,\n",
    "                             negative_prompt_embeds=base_negative_prompt_embeds if base_negative_prompt_embeds != \"\" and use_compel else None,\n",
    "                             negative_pooled_prompt_embeds=base_negative_prompt_pooled if base_negative_prompt_pooled != \"\" and use_compel else None,\n",
    "                             num_inference_steps=num_inference_steps,\n",
    "                             generator=generator,\n",
    "                             guidance_scale=guidance_scale,\n",
    "                             denoising_start=high_noise_frac_slider.value, \n",
    "                             image=base_output.images,\n",
    "                             #original_size = (height, width),\n",
    "                             #target_size = (height, width)\n",
    "                             ).images[0]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "display(image_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old but gold stuff\n",
    "\n",
    "#if use_refiner_checkbox.value:\n",
    "    #base_pipe.to(\"cpu\")\n",
    "    #torch.cuda.empty_cache()\n",
    "    #torch.cuda.ipc_collect() \n",
    "    #unrefined_image = postprocess_latent(base_pipe, base_output)\n",
    "    #display(unrefined_image)\n",
    "#else:\n",
    "   #display(base_output.images[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
