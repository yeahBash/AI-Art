{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# init\n",
    "import torch\n",
    "# configs\n",
    "from sdconfigs import SDXLConfig, UIData\n",
    "# utilities\n",
    "import utilities as utils\n",
    "import copy\n",
    "# compel helper\n",
    "from compel_helper import SDXLCompelHelper\n",
    "# for additional ui\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from ipywidgets import VBox, HBox\n",
    "from PIL import Image\n",
    "\n",
    "# hugging face cache directory\n",
    "CACHE_DIR = \"D:\\HuggingFaceCache\"\n",
    "# config init\n",
    "DEFAULT_CONFIG_PATH = \"sdxl_config.json\"\n",
    "def save_config_to_default(config:SDXLConfig): config.save_config(DEFAULT_CONFIG_PATH)\n",
    "last_config:SDXLConfig = None\n",
    "base_pipe = None\n",
    "refiner_pipe = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline and generate image methods\n",
    "def set_base_pipe_from_config(config:SDXLConfig):\n",
    "    global base_pipe\n",
    "    base_pipe = config.base_pipeline_type.from_pretrained(config.base_model, cache_dir=CACHE_DIR, \n",
    "                                                torch_dtype=config.torch_dtype,\n",
    "                                                variant=config.variant,\n",
    "                                                use_safetensors=config.use_safetensors)\n",
    "\n",
    "    scheduler = config.scheduler_type.from_config(base_pipe.scheduler.config, \n",
    "                                                use_karras_sigmas=config.use_karras_sigmas,\n",
    "                                                timestep_spacing=config.timestep_spacing)\n",
    "    base_pipe.scheduler = scheduler\n",
    "    #base_pipe.unet = torch.compile(base_pipe.unet, mode=\"reduce-overhead\", fullgraph=True) # not for windows\n",
    "\n",
    "    # base pipeline to CUDA\n",
    "    utils.to_cuda(base_pipe, \"Base -> CUDA started\", \"Base -> CUDA finished\")\n",
    "\n",
    "def set_refiner_pipe_from_config(config:SDXLConfig, base_pipe):\n",
    "    global refiner_pipe\n",
    "    refiner_pipe = config.refiner_pipeline_type.from_pretrained(config.refiner_model, cache_dir=CACHE_DIR, \n",
    "                                    torch_dtype=config.torch_dtype,\n",
    "                                    variant=config.variant,\n",
    "                                    use_safetensors=config.use_safetensors,\n",
    "                                    vae=base_pipe.vae,\n",
    "                                    text_encoder_2=base_pipe.text_encoder_2)\n",
    "\n",
    "    refiner_pipe.scheduler = base_pipe.scheduler\n",
    "    #refiner_pipe.unet = torch.compile(refiner_pipe.unet, mode=\"reduce-overhead\", fullgraph=True) # not for windows\n",
    "\n",
    "    # refiner pipeline to CUDA\n",
    "    utils.to_cuda(refiner_pipe, \"Refiner -> CUDA started\", \"Refiner -> CUDA finished\")\n",
    "\n",
    "def generate_refined_image_from_config(config:SDXLConfig, refiner_pipe, generator, prompt, prompt_2, negative_prompt, negative_prompt_2, base_output_images, use_ensemble_of_experts:bool) -> Image:\n",
    "    # prompt embeds\n",
    "    (base_positive_prompt_embeds_refiner, base_positive_prompt_pooled_refiner, base_negative_prompt_embeds_refiner, base_negative_prompt_pooled_refiner) = [None, None, None, None]\n",
    "    if config.use_compel:\n",
    "        # init compel\n",
    "        compel_refiner = SDXLCompelHelper(None, None, refiner_pipe.tokenizer_2, refiner_pipe.text_encoder_2)\n",
    "        (base_positive_prompt_embeds_refiner, base_positive_prompt_pooled_refiner, base_negative_prompt_embeds_refiner, base_negative_prompt_pooled_refiner) = compel_refiner.get_embeddings(config.prompt, config.prompt_2, config.negative_prompt, config.negative_prompt_2)\n",
    "\n",
    "    prompt_embeds=base_positive_prompt_embeds_refiner \n",
    "    pooled_prompt_embeds=base_positive_prompt_pooled_refiner\n",
    "    negative_prompt_embeds=base_negative_prompt_embeds_refiner\n",
    "    negative_pooled_prompt_embeds=base_negative_prompt_pooled_refiner\n",
    "\n",
    "    # denoising start\n",
    "    denoising_start=config.high_noise_frac if use_ensemble_of_experts else 0.0\n",
    "\n",
    "    # inference\n",
    "    base_output_refined = refiner_pipe(prompt=prompt,\n",
    "                                    prompt_2=prompt_2,\n",
    "                                    negative_prompt=negative_prompt,\n",
    "                                    negative_prompt_2=negative_prompt_2,\n",
    "                                    prompt_embeds=prompt_embeds,\n",
    "                                    pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                                    negative_prompt_embeds=negative_prompt_embeds,\n",
    "                                    negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "                                    num_inference_steps=config.num_inference_steps,\n",
    "                                    generator=generator,\n",
    "                                    guidance_scale=config.guidance_scale,\n",
    "                                    denoising_start=denoising_start, \n",
    "                                    image=base_output_images,\n",
    "                                    #original_size = (height, width),\n",
    "                                    #target_size = (height, width)\n",
    "                                    )\n",
    "    \n",
    "    return base_output_refined.images[0]\n",
    "\n",
    "def generate_image_from_config(config:SDXLConfig, update_pipe:bool) -> Image:\n",
    "    if base_pipe == None or update_pipe:\n",
    "        set_base_pipe_from_config(config)\n",
    "\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(config.seed)\n",
    "\n",
    "    # warnings\n",
    "    use_ensemble_of_experts:bool = True\n",
    "    if config.high_noise_frac == 1.0 and config.use_refiner:\n",
    "        use_ensemble_of_experts = False\n",
    "        print(\"High noise fraction == 1.0 and Use refiner is True -> Set use_ensemble_of_experts to False\")\n",
    "    elif config.high_noise_frac < 1.0 and not config.use_refiner:\n",
    "        print(\"High noise fraction < 1.0 and Use refiner is False\")\n",
    "\n",
    "    # prompt embeds\n",
    "    (base_positive_prompt_embeds, base_positive_prompt_pooled, base_negative_prompt_embeds, base_negative_prompt_pooled) = [None, None, None, None]\n",
    "    if config.use_compel:\n",
    "        # init compel\n",
    "        compel = SDXLCompelHelper(base_pipe.tokenizer, base_pipe.text_encoder, base_pipe.tokenizer_2, base_pipe.text_encoder_2)\n",
    "        (base_positive_prompt_embeds, base_positive_prompt_pooled, base_negative_prompt_embeds, base_negative_prompt_pooled) = compel.get_embeddings(config.prompt, config.prompt_2, config.negative_prompt, config.negative_prompt_2)\n",
    "\n",
    "    # prepare for inference\n",
    "    prompt=config.prompt if config.prompt != \"\" and not config.use_compel else None\n",
    "    prompt_2=config.prompt_2 if config.prompt_2 != \"\" and not config.use_compel else None\n",
    "    negative_prompt=config.negative_prompt if config.negative_prompt != \"\" and not config.use_compel else None\n",
    "    negative_prompt_2=config.negative_prompt_2 if config.negative_prompt_2 != \"\" and not config.use_compel else None\n",
    "    prompt_embeds=base_positive_prompt_embeds\n",
    "    pooled_prompt_embeds=base_positive_prompt_pooled\n",
    "    negative_prompt_embeds=base_negative_prompt_embeds\n",
    "    negative_pooled_prompt_embeds=base_negative_prompt_pooled\n",
    "    output_type=\"latent\" if config.use_refiner else \"pil\"\n",
    "\n",
    "    # save data to default config\n",
    "    save_config_to_default(config)\n",
    "\n",
    "    # inference\n",
    "    base_output = base_pipe(prompt=prompt,\n",
    "                                prompt_2=prompt_2,\n",
    "                                negative_prompt=negative_prompt,\n",
    "                                negative_prompt_2=negative_prompt_2,\n",
    "                                prompt_embeds=prompt_embeds,\n",
    "                                pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                                negative_prompt_embeds=negative_prompt_embeds,\n",
    "                                negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "                                num_inference_steps=config.num_inference_steps,\n",
    "                                generator=generator,\n",
    "                                guidance_scale=config.guidance_scale,\n",
    "                                output_type=output_type,\n",
    "                                denoising_end=config.high_noise_frac,\n",
    "                                #original_size = (height, width),\n",
    "                                #target_size = (height, width),\n",
    "                                **config.kwargs\n",
    "                                )\n",
    "\n",
    "    # results\n",
    "    if not config.use_refiner:\n",
    "        image = base_output.images[0]\n",
    "    else:\n",
    "        if refiner_pipe == None or update_pipe:\n",
    "            set_refiner_pipe_from_config(config, base_pipe)\n",
    "        image = generate_refined_image_from_config(config, refiner_pipe, generator, prompt, prompt_2, negative_prompt, negative_prompt_2, base_output.images, use_ensemble_of_experts)\n",
    "    \n",
    "    # save results\n",
    "    utils.save_results(image, DEFAULT_CONFIG_PATH, config.image, config.mask)\n",
    "\n",
    "    # clean gpu cache\n",
    "    torch.cuda.empty_cache()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "load_config_text_w = widgets.Text(value=DEFAULT_CONFIG_PATH, description='Load config from:', style={'description_width': 'initial'}, layout=Layout( width='auto'))\n",
    "display(load_config_text_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config ui\n",
    "def generate_image_to_ui():\n",
    "    generate_btn.disabled = True\n",
    "    global last_config\n",
    "    image = generate_image_from_config(config, last_config == None or not config.model_params_equals(last_config))\n",
    "    last_config = copy.deepcopy(config)\n",
    "    res_image_ui.value = utils.compressed_img_to_bytes(image, 'PNG')\n",
    "    generate_btn.disabled = False\n",
    "def on_prompt_text_area_changed(change): on_prompt_changed(change.new)\n",
    "def on_prompt_changed(value:str):\n",
    "    if config.is_turbo and value.endswith(\",\"):\n",
    "        generate_image_to_ui()\n",
    "\n",
    "load_config_path = load_config_text_w.value\n",
    "config:SDXLConfig = SDXLConfig.load_config(load_config_path)\n",
    "ui:UIData = config.get_ui()\n",
    "ui.prompt.observe(on_prompt_text_area_changed, names='value')\n",
    "\n",
    "# result image area\n",
    "image = Image.new(mode=\"RGB\", size=(512, 512))\n",
    "res_image_ui = widgets.Image(value=utils.compressed_img_to_bytes(image, 'PNG'), format='raw', layout=Layout())\n",
    "# save button\n",
    "def s(b): save_config_to_default(config) # for save btn\n",
    "save_btn = widgets.Button(description=\"Save config\", layout=Layout(width='50%'))\n",
    "save_btn.on_click(s)\n",
    "# generate button\n",
    "def g(b): generate_image_to_ui() # for generate btn\n",
    "generate_btn = widgets.Button(description=\"Generate\", layout=Layout(width='50%'))\n",
    "generate_btn.on_click(g)\n",
    "\n",
    "# display whole ui\n",
    "system_btns_box = HBox([generate_btn, save_btn], layout=Layout(margin='10px 0 0 0'))\n",
    "ui = HBox([VBox(ui.prompt_box, layout=Layout(width='33%', margin='10px 20px 10px 0')),\n",
    "           VBox(ui.params_box, layout=Layout(width='33%', margin='10px 0 10px 0')), \n",
    "           VBox([res_image_ui, system_btns_box], layout=Layout(width='33%', margin='10px 0 0 20px')),\n",
    "           ])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## old but gold stuff\n",
    "\n",
    "#if use_refiner_checkbox.value:\n",
    "    #base_pipe.to(\"cpu\")\n",
    "    #torch.cuda.empty_cache()\n",
    "    #torch.cuda.ipc_collect() \n",
    "    #unrefined_image = postprocess_latent(base_pipe, base_output)\n",
    "    #display(unrefined_image)\n",
    "#else:\n",
    "   #display(base_output.images[0])\n",
    "\n",
    "#right_box[f'{self.width=}'].layout.visibility = \"visible\" if value != \"StableDiffusionXLImg2ImgPipeline\" else \"hidden\"\n",
    "#right_box[f'{self.width=}'].layout.display = \"none\" if value != \"StableDiffusionXLImg2ImgPipeline\" else \"block\"\n",
    "#save_config_to_default(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
