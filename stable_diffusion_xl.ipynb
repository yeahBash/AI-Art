{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# init\n",
    "import torch\n",
    "\n",
    "# for parameters ui\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "# for config\n",
    "import json\n",
    "# for saving results\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "# for image_grid\n",
    "from PIL import Image\n",
    "# for prompt embeddings\n",
    "from compel import Compel, ReturnedEmbeddingsType\n",
    "\n",
    "# hugging face cache directory\n",
    "CACHE_DIR = \"D:\\HuggingFaceCache\"\n",
    "# config path\n",
    "CONFIG_PATH = \"sdxl_config.json\"\n",
    "\n",
    "# import models, schedulers and etc\n",
    "from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\n",
    "from diffusers import EulerDiscreteScheduler, DDIMScheduler, LMSDiscreteScheduler\n",
    "\n",
    "BASE_PIPELINES = {\"StableDiffusionXLPipeline\":StableDiffusionXLPipeline,}\n",
    "REFINER_PIPELINES = {\"StableDiffusionXLImg2ImgPipeline\":StableDiffusionXLImg2ImgPipeline}\n",
    "SCHEDULERS = {\"EulerDiscreteScheduler\":EulerDiscreteScheduler, \n",
    "              \"DDIMScheduler\":DDIMScheduler, \n",
    "              \"LMSDiscreteScheduler\":LMSDiscreteScheduler}\n",
    "PRECISION = {\"torch.float16\":torch.float16}\n",
    "\n",
    "class SDXLConfig:\n",
    "    def __init__(self,\n",
    "                 base_pipe_model: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "                 refiner_pipe_model: str = \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "                 torch_dtype_str: str = \"torch.float16\",\n",
    "                 base_pipeline_type_str: str = \"StableDiffusionXLPipeline\",\n",
    "                 refiner_pipeline_type_str: str = \"StableDiffusionXLImg2ImgPipeline\",\n",
    "                 scheduler_type_str: str = \"LMSDiscreteScheduler\",\n",
    "                 variant: str = \"fp16\",\n",
    "                 use_safetensors: bool = True,\n",
    "                 #safety_checker = None\n",
    "                 prompt: str = None,\n",
    "                 prompt_2: str = None,\n",
    "                 negative_prompt: str = None,\n",
    "                 negative_prompt_2: str = None,\n",
    "                 use_compel: bool = False,\n",
    "                 num_inference_steps: int = 40,\n",
    "                 width: int = 768,\n",
    "                 height: int = 768,\n",
    "                 guidance_scale: float = 7.5,\n",
    "                 high_noise_frac: float = 0.8,\n",
    "                 seed: int = 12345,\n",
    "                 use_refiner: bool = False\n",
    "                 ):\n",
    "        self.base_pipe_model = base_pipe_model\n",
    "        self.refiner_pipe_model = refiner_pipe_model\n",
    "        self.torch_dtype_str = torch_dtype_str\n",
    "        self.base_pipeline_type_str = base_pipeline_type_str\n",
    "        self.refiner_pipeline_type_str = refiner_pipeline_type_str\n",
    "        self.scheduler_type_str = scheduler_type_str\n",
    "        self.variant = variant\n",
    "        self.use_safetensors = use_safetensors\n",
    "        self.prompt = prompt\n",
    "        self.prompt_2 = prompt_2\n",
    "        self.negative_prompt = negative_prompt\n",
    "        self.negative_prompt_2 = negative_prompt_2\n",
    "        self.use_compel = use_compel\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.guidance_scale = guidance_scale\n",
    "        self.high_noise_frac = high_noise_frac\n",
    "        self.seed = seed\n",
    "        self.use_refiner = use_refiner\n",
    "\n",
    "    @property\n",
    "    def torch_dtype(self):return PRECISION[self.torch_dtype_str]\n",
    "    @property\n",
    "    def base_pipeline_type(self):return BASE_PIPELINES[self.base_pipeline_type_str]\n",
    "    @property\n",
    "    def refiner_pipeline_type(self):return REFINER_PIPELINES[self.refiner_pipeline_type_str]\n",
    "    @property\n",
    "    def scheduler_type(self):return SCHEDULERS[self.scheduler_type_str]\n",
    "    \n",
    "    def to_json(obj):\n",
    "        if isinstance(obj, SDXLConfig):\n",
    "            return obj.__dict__\n",
    "    def from_json(dict: dict):\n",
    "            return SDXLConfig(**dict)\n",
    "    def load_config():\n",
    "         with open(CONFIG_PATH, \"r\") as read_file:\n",
    "            return json.load(read_file, object_hook=SDXLConfig.from_json)\n",
    "    def save_config(self):\n",
    "         with open(CONFIG_PATH, \"w\") as write_file:\n",
    "            json.dump(self, write_file, skipkeys=True, indent=1, default=SDXLConfig.to_json)\n",
    "\n",
    "    def set_ui(self):\n",
    "        # TODO: not best workaround to get variable name\n",
    "        def f(x, f_name): setattr(self, f_name.split('=')[0].split('.')[1], x)\n",
    "\n",
    "        # models, precisions, schedulers\n",
    "        style = {'description_width': 'initial'}\n",
    "        interact(f, x=widgets.Text(value=self.base_pipe_model, placeholder='', description='Base model:', style=style), f_name=fixed(f'{self.base_pipe_model=}'))\n",
    "        interact(f, x=widgets.Text(value=self.refiner_pipe_model, placeholder='', description='Refiner model:', style=style), f_name=fixed(f'{self.refiner_pipe_model=}'))\n",
    "        interact(f, x=widgets.Dropdown(value=self.torch_dtype_str, options=PRECISION.keys(), description='dtype:', style=style), f_name=fixed(f'{self.torch_dtype_str=}'))\n",
    "        interact(f, x=widgets.Dropdown(value=self.base_pipeline_type_str, options=BASE_PIPELINES.keys(), description='Base type:', style=style), f_name=fixed(f'{self.base_pipeline_type_str=}'))\n",
    "        interact(f, x=widgets.Dropdown(value=self.refiner_pipeline_type_str, options=REFINER_PIPELINES.keys(), description='Refiner type:', style=style), f_name=fixed(f'{self.refiner_pipeline_type_str=}'))\n",
    "        interact(f, x=widgets.Dropdown(value=self.scheduler_type_str, options=SCHEDULERS.keys(), description='Scheduler type:', style=style), f_name=fixed(f'{self.scheduler_type_str=}'))\n",
    "        \n",
    "        # prompts\n",
    "        interact(f, x=widgets.Textarea(value=self.prompt, placeholder='Type positive1...', description='Prompt1:', style=style), f_name=fixed(f'{self.prompt=}'))\n",
    "        interact(f, x=widgets.Textarea(value=self.prompt_2, placeholder='Type positive2...', description='Prompt2:', style=style), f_name=fixed(f'{self.prompt_2=}'))\n",
    "        interact(f, x=widgets.Textarea(value=self.negative_prompt, placeholder='Type negative1...', description='Negative Prompt1:', style = style), f_name=fixed(f'{self.negative_prompt=}'))\n",
    "        interact(f, x=widgets.Textarea(value=self.negative_prompt_2, placeholder='Type negative2...', description='Negative Prompt2:', style = style), f_name=fixed(f'{self.negative_prompt_2=}'))\n",
    "        interact(f, x=widgets.Checkbox(value=self.use_compel, description=\"Use Compel\", indent=False, style=style), f_name=fixed(f'{self.use_compel=}'))\n",
    "\n",
    "        # inference properties\n",
    "        interact(f, x=widgets.IntSlider(value=self.num_inference_steps, min=10, max=100, step=5, description=\"Num inference steps:\", continuous_update=False, style=style), f_name=fixed(f'{self.num_inference_steps=}'))\n",
    "        interact(f, x=widgets.IntSlider(value=self.width, min=512, max=1024, step=64, description=\"Width:\", continuous_update=False, style=style), f_name=fixed(f'{self.width=}'))\n",
    "        interact(f, x=widgets.FloatSlider(value=self.guidance_scale, min=0, max=10, step=0.25, description=\"Guidance scale:\", continuous_update=False, style=style), f_name=fixed(f'{self.guidance_scale=}'))\n",
    "        interact(f, x=widgets.IntSlider(value=self.seed, min=0, max=1000000, step=1, description=\"Seed:\", continuous_update=False, style=style), f_name=fixed(f'{self.seed=}'))\n",
    "        interact(f, x=widgets.FloatSlider(value=self.high_noise_frac, min=0, max=1, step=0.05, description=\"High noise frac:\", continuous_update=False, style=style), f_name=fixed(f'{self.high_noise_frac=}'))\n",
    "\n",
    "        # refiner\n",
    "        interact(f, x=widgets.Checkbox(value=self.use_refiner, description=\"Use refiner\", indent=False, style=style), f_name=fixed(f'{self.use_refiner=}'))\n",
    "\n",
    "# utilities methods\n",
    "def to_cuda(pipe, start_mess, end_mess):\n",
    "    if(torch.cuda.is_available()):\n",
    "        print(start_mess)\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA IS NOT AVAILABLE\")\n",
    "    print(end_mess)\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def postprocess_latent(pipe, latent):\n",
    "    vae_output = pipe.vae.decode(\n",
    "        latent.images / pipe.vae.config.scaling_factor, return_dict=False\n",
    "    )[0].detach()\n",
    "    return pipe.image_processor.postprocess(vae_output, output_type=\"pil\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ui\n",
    "config: SDXLConfig = SDXLConfig.load_config()\n",
    "config.set_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline, set schelduler, to cuda, compel init and etc\n",
    "base_pipe_model = config.base_pipeline_type.from_pretrained(config.base_pipe_model, cache_dir=CACHE_DIR, \n",
    "                                               torch_dtype=config.torch_dtype,\n",
    "                                               variant=config.variant,\n",
    "                                               use_safetensors=config.use_safetensors)\n",
    "\n",
    "\n",
    "scheduler = config.scheduler_type.from_config(base_pipe_model.scheduler.config)\n",
    "base_pipe_model.scheduler = scheduler\n",
    "#base_pipe.unet = torch.compile(base_pipe.unet, mode=\"reduce-overhead\", fullgraph=True) # not for windows\n",
    "\n",
    "# base pipeline to CUDA\n",
    "to_cuda(base_pipe_model, \"Base -> CUDA started\", \"Base -> CUDA finished\")\n",
    "\n",
    "# Compels init (TODO: maybe should move \"generate image\" cell)\n",
    "base_compel_1 = Compel(\n",
    "    tokenizer=base_pipe_model.tokenizer,\n",
    "    text_encoder=base_pipe_model.text_encoder,\n",
    "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "    requires_pooled=False,\n",
    ")\n",
    "base_compel_2 = Compel(\n",
    "    tokenizer=base_pipe_model.tokenizer_2,\n",
    "    text_encoder=base_pipe_model.text_encoder_2,\n",
    "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
    "    requires_pooled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate image\n",
    "\n",
    "# set embeds\n",
    "base_positive_prompt_embeds_1 = base_compel_1(config.prompt)\n",
    "base_positive_prompt_embeds_2, base_positive_prompt_pooled = base_compel_2(config.prompt_2)\n",
    "base_negative_prompt_embeds_1 = base_compel_1(config.negative_prompt)\n",
    "base_negative_prompt_embeds_2, base_negative_prompt_pooled = base_compel_2(config.negative_prompt_2)\n",
    "\n",
    "# Pad the conditioning tensors to ensure thet they all have the same length\n",
    "(base_positive_prompt_embeds_2, base_negative_prompt_embeds_2) = base_compel_2.pad_conditioning_tensors_to_same_length([base_positive_prompt_embeds_2, base_negative_prompt_embeds_2])\n",
    "\n",
    "# Concatenate the cconditioning tensors corresponding to both the set of prompts\n",
    "base_positive_prompt_embeds = torch.cat((base_positive_prompt_embeds_1, base_positive_prompt_embeds_2), dim=-1)\n",
    "base_negative_prompt_embeds = torch.cat((base_negative_prompt_embeds_1, base_negative_prompt_embeds_2), dim=-1)\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(config.seed)\n",
    "\n",
    "# base\n",
    "prompt = config.prompt if config.prompt != \"\" and not config.use_compel else None\n",
    "prompt_2 = config.prompt_2  if config.prompt_2 != \"\" and not config.use_compel else None\n",
    "negative_prompt = config.negative_prompt if config.negative_prompt != \"\" and not config.use_compel else None\n",
    "negative_prompt_2 = config.negative_prompt_2 if config.negative_prompt_2 != \"\" and not config.use_compel else None\n",
    "prompt_embeds=base_positive_prompt_embeds if base_positive_prompt_embeds != \"\" and config.use_compel else None\n",
    "pooled_prompt_embeds=base_positive_prompt_pooled if base_positive_prompt_pooled != \"\" and config.use_compel else None\n",
    "negative_prompt_embeds=base_negative_prompt_embeds if base_negative_prompt_embeds != \"\" and config.use_compel else None\n",
    "negative_pooled_prompt_embeds=base_negative_prompt_pooled if base_negative_prompt_pooled != \"\" and config.use_compel else None\n",
    "output_type=\"latent\" if config.use_refiner else \"pil\"\n",
    "\n",
    "print(f\"{prompt}\\n{prompt_2}\\n{negative_prompt}\\n{negative_prompt_2}\")\n",
    "base_output = base_pipe_model(prompt=prompt,\n",
    "                              prompt_2=prompt_2,\n",
    "                              negative_prompt=negative_prompt,\n",
    "                              negative_prompt_2=negative_prompt_2,\n",
    "                              prompt_embeds=prompt_embeds,\n",
    "                              pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                              negative_prompt_embeds=negative_prompt_embeds,\n",
    "                              negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "                              num_inference_steps=config.num_inference_steps,\n",
    "                              generator=generator,\n",
    "                              guidance_scale=config.guidance_scale,\n",
    "                              output_type=output_type,\n",
    "                              denoising_end=config.high_noise_frac,\n",
    "                              width=config.width,\n",
    "                              height=config.height)\n",
    "\n",
    "# save data to config\n",
    "config.save_config()\n",
    "\n",
    "# save results\n",
    "if not config.use_refiner:\n",
    "    cur_date = datetime.now().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "    image = base_output.images[0]\n",
    "    shutil.copyfile(\"sdxl_config.json\", f\"results\\{cur_date}.json\")\n",
    "    image.save(f\"results\\{cur_date}.png\")\n",
    "    display(image)\n",
    "    \n",
    "# clean gpu cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refiner\n",
    "\n",
    "refiner_pipe_model = refiner_pipeline_type.from_pretrained(refiner_model, cache_dir=CACHE_DIR, \n",
    "                                    torch_dtype=torch_dtype,\n",
    "                                    variant=variant,\n",
    "                                    use_safetensors=use_safetensors,\n",
    "                                    vae=base_pipe_model.vae,\n",
    "                                    text_encoder_2=base_pipe_model.text_encoder_2)\n",
    "\n",
    "refiner_pipe_model.scheduler = scheduler\n",
    "#refiner_pipe.unet = torch.compile(refiner_pipe.unet, mode=\"reduce-overhead\", fullgraph=True) # not for windows\n",
    "\n",
    "# refiner pipeline to CUDA\n",
    "to_cuda(base_pipe_model, \"Refiner -> CUDA started\", \"Refiner -> CUDA finished\")\n",
    "\n",
    "\n",
    "image_refined = refiner_pipe_model(prompt=prompt if prompt != \"\" and not use_compel else None,\n",
    "                             prompt_2 = prompt_2 if prompt_2 != \"\" and not use_compel else None,\n",
    "                             negative_prompt = negative_prompt if negative_prompt != \"\" and not use_compel else None,\n",
    "                             negative_prompt_2 = negative_prompt_2 if negative_prompt_2 != \"\" and not use_compel else None,\n",
    "                             prompt_embeds=base_positive_prompt_embeds if base_positive_prompt_embeds != \"\" and use_compel else None,\n",
    "                             pooled_prompt_embeds=base_positive_prompt_pooled if base_positive_prompt_pooled != \"\" and use_compel else None,\n",
    "                             negative_prompt_embeds=base_negative_prompt_embeds if base_negative_prompt_embeds != \"\" and use_compel else None,\n",
    "                             negative_pooled_prompt_embeds=base_negative_prompt_pooled if base_negative_prompt_pooled != \"\" and use_compel else None,\n",
    "                             num_inference_steps=num_inference_steps,\n",
    "                             generator=generator,\n",
    "                             guidance_scale=guidance_scale,\n",
    "                             denoising_start=high_noise_frac_slider.value, \n",
    "                             image=base_output.images,\n",
    "                             #original_size = (height, width),\n",
    "                             #target_size = (height, width)\n",
    "                             ).images[0]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "display(image_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old but gold stuff\n",
    "\n",
    "#if use_refiner_checkbox.value:\n",
    "    #base_pipe.to(\"cpu\")\n",
    "    #torch.cuda.empty_cache()\n",
    "    #torch.cuda.ipc_collect() \n",
    "    #unrefined_image = postprocess_latent(base_pipe, base_output)\n",
    "    #display(unrefined_image)\n",
    "#else:\n",
    "   #display(base_output.images[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
