{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# init\n",
    "import torch\n",
    "\n",
    "# configs\n",
    "from sdconfigs import SDXLConfig\n",
    "# utilities\n",
    "import utilities as utils\n",
    "# compel helper\n",
    "from compel_helper import SDXLCompelHelper\n",
    "\n",
    "# hugging face cache directory\n",
    "CACHE_DIR = \"D:\\HuggingFaceCache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ui\n",
    "config: SDXLConfig = SDXLConfig.load_config()\n",
    "config.set_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline, set schelduler, to cuda, compel init and etc\n",
    "base_pipe_model = config.base_pipeline_type.from_pretrained(config.base_pipe_model, cache_dir=CACHE_DIR, \n",
    "                                               torch_dtype=config.torch_dtype,\n",
    "                                               variant=config.variant,\n",
    "                                               use_safetensors=config.use_safetensors)\n",
    "\n",
    "\n",
    "scheduler = config.scheduler_type.from_config(base_pipe_model.scheduler.config)\n",
    "base_pipe_model.scheduler = scheduler\n",
    "#base_pipe.unet = torch.compile(base_pipe.unet, mode=\"reduce-overhead\", fullgraph=True) # not for windows\n",
    "\n",
    "# init compel\n",
    "compel = SDXLCompelHelper(base_pipe_model.tokenizer, base_pipe_model.text_encoder, base_pipe_model.tokenizer_2, base_pipe_model.text_encoder_2)\n",
    "# base pipeline to CUDA\n",
    "utils.to_cuda(base_pipe_model, \"Base -> CUDA started\", \"Base -> CUDA finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate image\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(config.seed)\n",
    "if config.high_noise_frac == 1.0 and config.use_refiner:\n",
    "    print(\"High noise fraction == 1.0 and Use refiner is True\")\n",
    "\n",
    "# prompt embeds\n",
    "(base_positive_prompt_embeds, base_positive_prompt_pooled, base_negative_prompt_embeds, base_negative_prompt_pooled) = compel.get_embeddings(config.prompt, config.prompt_2, config.negative_prompt, config.negative_prompt_2)\n",
    "\n",
    "# prepare for inference\n",
    "prompt = config.prompt if config.prompt != \"\" and not config.use_compel else None\n",
    "prompt_2 = config.prompt_2  if config.prompt_2 != \"\" and not config.use_compel else None\n",
    "negative_prompt = config.negative_prompt if config.negative_prompt != \"\" and not config.use_compel else None\n",
    "negative_prompt_2 = config.negative_prompt_2 if config.negative_prompt_2 != \"\" and not config.use_compel else None\n",
    "prompt_embeds = base_positive_prompt_embeds if config.use_compel else None\n",
    "pooled_prompt_embeds=base_positive_prompt_pooled if config.use_compel else None\n",
    "negative_prompt_embeds=base_negative_prompt_embeds if config.use_compel else None\n",
    "negative_pooled_prompt_embeds=base_negative_prompt_pooled if config.use_compel else None\n",
    "output_type=\"latent\" if config.use_refiner else \"pil\"\n",
    "\n",
    "# base\n",
    "print(f\"{config.prompt}\\n{config.prompt_2}\\n{config.negative_prompt}\\n{config.negative_prompt_2}\")\n",
    "base_output = base_pipe_model(prompt=prompt,\n",
    "                              prompt_2=prompt_2,\n",
    "                              negative_prompt=negative_prompt,\n",
    "                              negative_prompt_2=negative_prompt_2,\n",
    "                              prompt_embeds=prompt_embeds,\n",
    "                              pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                              negative_prompt_embeds=negative_prompt_embeds,\n",
    "                              negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "                              num_inference_steps=config.num_inference_steps,\n",
    "                              generator=generator,\n",
    "                              guidance_scale=config.guidance_scale,\n",
    "                              output_type=output_type,\n",
    "                              denoising_end=config.high_noise_frac,\n",
    "                              width=config.width,\n",
    "                              height=config.height)\n",
    "\n",
    "# save data to config\n",
    "config.save_config()\n",
    "\n",
    "# save results\n",
    "if not config.use_refiner:\n",
    "    image = base_output.images[0]\n",
    "    utils.save_results(image, SDXLConfig.CONFIG_PATH)\n",
    "    display(image)\n",
    "    \n",
    "# clean gpu cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refiner\n",
    "\n",
    "refiner_pipe_model = config.refiner_pipeline_type.from_pretrained(config.refiner_pipe_model, cache_dir=CACHE_DIR, \n",
    "                                    torch_dtype=config.torch_dtype,\n",
    "                                    variant=config.variant,\n",
    "                                    use_safetensors=config.use_safetensors,\n",
    "                                    vae=base_pipe_model.vae,\n",
    "                                    text_encoder_2=base_pipe_model.text_encoder_2)\n",
    "\n",
    "refiner_pipe_model.scheduler = scheduler\n",
    "#refiner_pipe.unet = torch.compile(refiner_pipe.unet, mode=\"reduce-overhead\", fullgraph=True) # not for windows\n",
    "\n",
    "# refiner pipeline to CUDA\n",
    "utils.to_cuda(refiner_pipe_model, \"Refiner -> CUDA started\", \"Refiner -> CUDA finished\")\n",
    "\n",
    "# prompt embeds\n",
    "compel_refiner = SDXLCompelHelper(None, None, refiner_pipe_model.tokenizer_2, refiner_pipe_model.text_encoder_2)\n",
    "(base_positive_prompt_embeds_refiner, base_positive_prompt_pooled_refiner, base_negative_prompt_embeds_refiner, base_negative_prompt_pooled_refiner) = compel_refiner.get_embeddings(config.prompt, config.prompt_2, config.negative_prompt, config.negative_prompt_2)\n",
    "\n",
    "prompt_embeds = base_positive_prompt_embeds_refiner if config.use_compel else None\n",
    "pooled_prompt_embeds=base_positive_prompt_pooled_refiner if config.use_compel else None\n",
    "negative_prompt_embeds=base_negative_prompt_embeds_refiner if config.use_compel else None\n",
    "negative_pooled_prompt_embeds=base_negative_prompt_pooled_refiner if config.use_compel else None\n",
    "\n",
    "image_refined = refiner_pipe_model(prompt=prompt,\n",
    "                             prompt_2 = prompt_2,\n",
    "                             negative_prompt = negative_prompt,\n",
    "                             negative_prompt_2 = negative_prompt_2,\n",
    "                             prompt_embeds=prompt_embeds,\n",
    "                             pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                             negative_prompt_embeds=negative_prompt_embeds,\n",
    "                             negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "                             num_inference_steps=config.num_inference_steps,\n",
    "                             generator=generator,\n",
    "                             guidance_scale=config.guidance_scale,\n",
    "                             denoising_start=config.high_noise_frac, \n",
    "                             image=base_output.images,\n",
    "                             #original_size = (height, width),\n",
    "                             #target_size = (height, width)\n",
    "                             ).images[0]\n",
    "\n",
    "utils.save_results(image_refined, SDXLConfig.CONFIG_PATH)\n",
    "display(image_refined)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## old but gold stuff\n",
    "\n",
    "#if use_refiner_checkbox.value:\n",
    "    #base_pipe.to(\"cpu\")\n",
    "    #torch.cuda.empty_cache()\n",
    "    #torch.cuda.ipc_collect() \n",
    "    #unrefined_image = postprocess_latent(base_pipe, base_output)\n",
    "    #display(unrefined_image)\n",
    "#else:\n",
    "   #display(base_output.images[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
